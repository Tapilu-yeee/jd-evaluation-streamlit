import streamlit as st
import google.generativeai as genai
import docx
import os

# Cáº¥u hÃ¬nh API Key tá»« secrets
api_key = api_key = "AIzaSyALIFJZAmvuu5G5QVOMjp0bXb7sn-Hhfh4"
genai.configure(api_key=api_key)

# HÃ m Ä‘á»c ná»™i dung file .docx
def read_docx(file):
    doc = docx.Document(file)
    return "\n".join([para.text for para in doc.paragraphs if para.text.strip() != ""])

# Load phÆ°Æ¡ng phÃ¡p PwC tá»« file text
@st.cache_data
def load_pwc_prompt():
    with open("pwc_prompt.txt", "r", encoding="utf-8") as f:
        return f.read()

# Khá»Ÿi táº¡o model Gemini
model = genai.GenerativeModel("gemini-pro")

# Giao diá»‡n Streamlit
st.set_page_config(page_title="ÄÃ¡nh giÃ¡ giÃ¡ trá»‹ cÃ´ng viá»‡c (PwC)", layout="wide")
st.title("ğŸ“‹ ÄÃ¡nh giÃ¡ mÃ´ táº£ cÃ´ng viá»‡c theo 12 yáº¿u tá»‘ PwC")
st.markdown("Táº£i lÃªn mÃ´ táº£ cÃ´ng viá»‡c (.docx) Ä‘á»ƒ há»‡ thá»‘ng thá»±c hiá»‡n Ä‘Ã¡nh giÃ¡.")

# Nháº­p tÃªn vá»‹ trÃ­
job_title = st.text_input("ğŸ”¤ Nháº­p tÃªn vá»‹ trÃ­ cÃ´ng viá»‡c:")

# Upload file mÃ´ táº£ cÃ´ng viá»‡c
uploaded_file = st.file_uploader("ğŸ“ Táº£i lÃªn JD Ä‘á»‹nh dáº¡ng .docx", type=["docx", "txt"])

if uploaded_file and job_title:
    with st.spinner("ğŸ§  Äang phÃ¢n tÃ­ch vÃ  Ä‘Ã¡nh giÃ¡..."):
        # jd_content = read_docx(uploaded_file)
        stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
        jd_content = stringio.read()
        full_prompt = load_pwc_prompt() + f"\n\nÄÃ¢y lÃ  JD cho vá»‹ trÃ­: {job_title}\n\n{jd_content}"
        response = model.generate_content(full_prompt)
        result = response.text

        # Hiá»ƒn thá»‹ káº¿t quáº£
        st.markdown("### âœ… Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ theo 12 yáº¿u tá»‘ PwC")
        st.markdown(result)

        # LÆ°u JD Ä‘Ã£ Ä‘Ã¡nh giÃ¡ Ä‘á»ƒ so sÃ¡nh sau
        # if "jd_history" not in st.session_state:
        if not hasattr(st.session_state, "jd_history"):
            st.session_state.jd_history = []

        st.session_state.jd_history.append({"position": job_title, "content": jd_content})

        # So sÃ¡nh JD má»›i vá»›i cÃ¡c JD Ä‘Ã£ upload trÆ°á»›c Ä‘Ã³
if hasattr(st.session_state, "jd_history") and len(st.session_state.jd_history) > 1:
    st.subheader("ğŸ” So sÃ¡nh pháº¡m vi cÃ´ng viá»‡c vá»›i cÃ¡c JD trÆ°á»›c")

    current_text = content_text.lower()
    similarity_results = []
    for item in st.session_state.jd_history:
        previous_title = item["title"]
        previous_text = item["text"].lower()

        # TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng báº±ng cosine similarity Ä‘Æ¡n giáº£n
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity

        vectorizer = TfidfVectorizer().fit_transform([current_text, previous_text])
        similarity = cosine_similarity(vectorizer[0:1], vectorizer[1:2])[0][0]

        similarity_results.append((previous_title, round(similarity * 100, 2)))

    # Hiá»ƒn thá»‹ káº¿t quáº£
    st.write("ğŸ” **So sÃ¡nh má»©c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá» pháº¡m vi cÃ´ng viá»‡c:**")
    for title, sim in sorted(similarity_results, key=lambda x: -x[1]):
        st.markdown(f"- **{title}** â†’ `{sim}%` tÆ°Æ¡ng Ä‘á»“ng")

else:
    st.info("ChÆ°a cÃ³ JD nÃ o Ä‘Æ°á»£c lÆ°u trÆ°á»›c Ä‘Ã³ Ä‘á»ƒ so sÃ¡nh.")


